{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import itertools\n",
    "import time\n",
    "\n",
    "\n",
    "from lake_envs import *\n",
    "\n",
    "\n",
    "def render_single(env, policy, max_steps=100):\n",
    "    \"\"\"\n",
    "    This function does not need to be modified\n",
    "    Renders policy once on environment. Watch your agent play!\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env: gym.core.Environment\n",
    "      Environment to play on. Must have nS, nA, and P as\n",
    "      attributes.\n",
    "    Policy: np.array of shape [env.nS]\n",
    "      The action to take at a given state\n",
    "    \"\"\"\n",
    "    episode_reward = 0\n",
    "    ob = env.reset()\n",
    "    for t in range(max_steps):\n",
    "        env.render()\n",
    "        time.sleep(0.25)\n",
    "        a = policy[ob]\n",
    "        ob, rew, done, _ = env.step(a)\n",
    "        episode_reward += rew\n",
    "        if done:\n",
    "            break\n",
    "    env.render();\n",
    "    if not done:\n",
    "        print(\"The agent didn't reach a terminal state in {} steps.\".format(max_steps))\n",
    "    else:\n",
    "        print(\"Episode reward: %f\" % episode_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCControl:\n",
    "    def __init__(self, epsilon, gamma, env, num_states, num_actions):\n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = gamma\n",
    "        self.env = env\n",
    "        self.num_states = num_states\n",
    "        self.num_actions = num_actions\n",
    "\n",
    "\n",
    "    def init_agent(self):\n",
    "        # check the documentation for this function: https://docs.scipy.org/doc//numpy-1.10.4/reference/generated/numpy.random.choice.html\n",
    "        self.policy = np.random.choice(?, ?)\n",
    "\n",
    "        self.Q = {}\n",
    "        self.visit_count = {}\n",
    "\n",
    "        for state in range(self.num_states):\n",
    "            self.Q[state] = {}\n",
    "            self.visit_count[state] = {}\n",
    "            for action in range(self.num_actions):\n",
    "                # YOUR CODE IS HERE\n",
    "                # Initialise Q values arbitrarily\n",
    "                self.visit_count[state][action] = 0\n",
    "\n",
    "\n",
    "    def get_epsilon_greedy_action(self, greedy_action):\n",
    "        prob = np.random.random()\n",
    "\n",
    "        if prob < 1 - self.epsilon:\n",
    "            return ???\n",
    "\n",
    "        return np.random.randint(0, self.num_actions)\n",
    "\n",
    "\n",
    "    def generate_episode(self, policy):\n",
    "        G = 0\n",
    "        s = env.reset()\n",
    "        a = self.get_epsilon_greedy_action(policy[s])\n",
    "\n",
    "        state_action_reward = [(s, a, 0)]\n",
    "        while True:\n",
    "            s, r, terminated, _ = env.step(a)\n",
    "            if terminated:\n",
    "                state_action_reward.append((s, None, r))\n",
    "                break\n",
    "            else:\n",
    "                a = self.get_epsilon_greedy_action(policy[s])\n",
    "                state_action_reward.append((s, a, r))\n",
    "\n",
    "        t = 1\n",
    "        for _, _, reward in state_action_reward:\n",
    "            # YOUR CODE IS HERE, implement G\n",
    "            G = \n",
    "            t += 1\n",
    "\n",
    "        return G, state_action_reward[:-1]\n",
    "\n",
    "    def argmax(self, Q, policy):\n",
    "        \"\"\"\n",
    "        Finds and returns greedy policy.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        Q: nested dictionary {state: {action: q value}}\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        policy: The action to take at a given state, list of length num_state\n",
    "\n",
    "        \"\"\"\n",
    "        for state in range(self.num_states):\n",
    "            best_action = None\n",
    "            best_value = float('-inf')\n",
    "\n",
    "            for action, value in Q[state].items():\n",
    "                # YOUR CODE HERE\n",
    "                # Complete the code to find best_action per state\n",
    "            policy[state] = best_action\n",
    "\n",
    "        return policy\n",
    "\n",
    "    def evaluate_policy(self, G, visit_counts, step, action):\n",
    "        # YOUR CODE IS HERE\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "    def improve_policy(self, Q, policy):\n",
    "        self.policy = self.argmax(Q, policy)\n",
    "\n",
    "\n",
    "    def run_mc_control(self, num_episodes):\n",
    "        self.init_agent()\n",
    "\n",
    "        for episode in range(num_episodes):\n",
    "            G, state_action_reward = self.generate_episode(self.policy)\n",
    "            seen_state_action = set()\n",
    "\n",
    "            for state, action, _ in state_action_reward:\n",
    "                #  if we see step and action pair for a first time in episode\n",
    "                if (state, action) not in seen_state_action:\n",
    "                    self.visit_count[state][action] += 1\n",
    "\n",
    "                    self.evaluate_policy(G, self.visit_count, state, action)\n",
    "\n",
    "                    seen_state_action.add((state, action))\n",
    "\n",
    "            self.improve_policy(self.Q, self.policy)\n",
    "\n",
    "        print (f\"Finished training RL agent for {num_episodes} episodes!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.4\n",
    "gamma = 0.9\n",
    "n_episodes = 10000\n",
    "\n",
    "# Init environment\n",
    "env = gym.make(\"Deterministic-4x4-FrozenLake-v0\")\n",
    "#env = gym.make(\"Stochastic-4x4-FrozenLake-v0\")\n",
    "\n",
    "num_states = env.nS\n",
    "num_actions = env.nA\n",
    "\n",
    "mc_model = MCControl(epsilon, gamma, env, num_states, num_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training RL agent for 10000 episodes!\n"
     ]
    }
   ],
   "source": [
    "mc_model.run_mc_control(n_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "render_single(env, mc_model.policy, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
