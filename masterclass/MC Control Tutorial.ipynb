{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Control from scratch in Python and solving Frozen Lake problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook you will:  \n",
    "\n",
    "1. Implement on-policy first-visit Monte Carlo Control with $\\epsilon$-greedy action selection.  \n",
    "2. Test MC Control on Frozen Lake problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Monte Carlo Control Pseudocode**:\n",
    "    \n",
    "    \n",
    "Input:  $epsilon$, $gamma$, $n\\_episodes$\n",
    "\n",
    "\n",
    "Initialize for all $s\\in S$ and $a\\in A$:    \n",
    ">$Q(s, a)$ <- arbitrary  \n",
    "    $\\pi(s)$ <- arbitrary\n",
    "\n",
    "Repeat for $n\\_episodes$:  \n",
    ">generate episode following $\\epsilon$-greedy policy  \n",
    "    $Q(s, a)$ <- evaluate policy using first-visit MC method   \n",
    "    $\\pi$ <- improve policy greedily\n",
    "\n",
    " \n",
    "$Q^*(s, a)$ <- $Q(s, a)$  \n",
    "$\\pi^*$  <- $\\pi$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import itertools\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MC Control Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create class called MC control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCControl:\n",
    "    '''Implements Monte Carlo Control.'''\n",
    "    def __init__(self, env, num_states, num_actions, epsilon, gamma):\n",
    "        '''Parameters\n",
    "        ----------\n",
    "        env:         open gym environment object\n",
    "        num_states:  integer, number of states in the environment\n",
    "        num_actions: integer, number of possible actions\n",
    "        epsilon:     float, the epsilon parameter used for exploration\n",
    "        gamma:       float, discount factor\n",
    "        '''\n",
    "        self.env = env\n",
    "        self.num_states = num_states\n",
    "        self.num_actions = num_actions\n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def init_agent(self):\n",
    "        '''Initializes RL agent components:\n",
    "        self.policy:      list of integers of length self.num_states, the action to take at a given state\n",
    "        self.Q:           nested dictionary {state: {action: q value}}, action value function\n",
    "        self.visit_count: nested dictionary {state: {action: count}}, keeps track of how many episodes\n",
    "                          state and action pair were visited for a first time in every episode\n",
    "        '''\n",
    "        # --------------------------\n",
    "        # Randomly initialize policy, use numpy random.choice method:\n",
    "        # your code here (1 line)\n",
    "        self.policy = np.random.choice(num_actions, num_states)\n",
    "        # --------------------------\n",
    "\n",
    "        self.Q = {}\n",
    "        self.visit_count = {}\n",
    "\n",
    "        for state in range(self.num_states):\n",
    "            self.Q[state] = {}\n",
    "            self.visit_count[state] = {}\n",
    "            for action in range(self.num_actions):\n",
    "                # --------------------------\n",
    "                # Initalize action value (self.Q) and visit count (self.visit_count) dictionaries to zero:\n",
    "                # your code here (~ 2 lines)\n",
    "                self.Q[state][action] = 0\n",
    "                self.visit_count[state][action] = 0\n",
    "                # --------------------------\n",
    "\n",
    "\n",
    "    def generate_episode(self, policy):\n",
    "        '''Generates episode given current policy.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        policy: list of integers of length self.num_states, the action to take at a given state\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        G: float, episode return (total discounted reward)\n",
    "        state_action_reward: list of tuple (state, action, reward), excludes terminal one\n",
    "        '''\n",
    "        G = 0\n",
    "        s = env.reset()\n",
    "        a = self.get_epsilon_greedy_action(policy[s])\n",
    "\n",
    "        state_action_reward = [(s, a, 0)]\n",
    "        while True:\n",
    "            s, r, terminated, _ = env.step(a)\n",
    "            if terminated:\n",
    "                state_action_reward.append((s, None, r))\n",
    "                break\n",
    "            else:\n",
    "                a = self.get_epsilon_greedy_action(policy[s])\n",
    "                state_action_reward.append((s, a, r))\n",
    "\n",
    "        # --------------------------\n",
    "        # Calculate G:\n",
    "        # your code here (~ 4 lines)\n",
    "        t = 1\n",
    "        for _, _, reward in state_action_reward:\n",
    "            G += self.gamma ** (t - 1) * reward\n",
    "            t += 1\n",
    "        # --------------------------\n",
    "\n",
    "        return G, state_action_reward[:-1]\n",
    "\n",
    "    def argmax(self, Q, policy):\n",
    "        \"\"\"\n",
    "        Finds and returns greedy policy.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        Q: nested dictionary {state: {action: q value}}, action value function\n",
    "        policy: list of integers of length self.num_states containing last actions per state\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        next_policy: list of integers of length self.num_states containing next actions with a highest value per state \n",
    "\n",
    "        \"\"\"\n",
    "        next_policy = policy\n",
    "        \n",
    "        for state in range(self.num_states):\n",
    "            best_action = None\n",
    "            best_value = float('-inf')\n",
    "            # --------------------------\n",
    "            # Find greedy action to take in every state and assign to policy[state]:\n",
    "            # your code here (~ 5 lines)\n",
    "            for action, value in Q[state].items():\n",
    "                if value > best_value:\n",
    "                    best_value = value\n",
    "                    best_action = action\n",
    "            next_policy[state] = best_action\n",
    "            # --------------------------\n",
    "\n",
    "        return next_policy\n",
    "\n",
    "    def get_epsilon_greedy_action(self, greedy_action):\n",
    "        '''Returns next action using epsilon greedy approach.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        greedy_action: integer, greedy action (action with a maximum Q value)\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        next_action: integer, either greedy or random action\n",
    "        '''   \n",
    "        prob = np.random.random()\n",
    "\n",
    "        if prob < 1 - self.epsilon:\n",
    "            return greedy_action\n",
    "\n",
    "        return np.random.randint(0, self.num_actions)\n",
    "    \n",
    "    def improve_policy(self):\n",
    "        '''Improves and updates current policy self.policy using epsilon greedy approach.'''\n",
    "        self.policy = self.argmax(self.Q, self.policy)\n",
    "        \n",
    "    def evaluate_policy(self, G, state_action_reward):\n",
    "        '''Evaluates current policy using incremental mean and updates action value function self.Q.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        G: float, episode return (total discounted reward)\n",
    "        state_action_reward: list of tuple (state, action, reward)\n",
    "        '''\n",
    "        seen_state_action = set()\n",
    "\n",
    "        for state, action, _ in state_action_reward:\n",
    "            #  if we see step and action pair for a first time in episode\n",
    "            if (state, action) not in seen_state_action:\n",
    "                self.visit_count[state][action] += 1\n",
    "                # --------------------------\n",
    "                # Calculate action value for current state and action\n",
    "                # your code here (1 line)\n",
    "                self.Q[state][action] += (G - self.Q[state][action]) / self.visit_count[state][action]\n",
    "                # --------------------------\n",
    "                seen_state_action.add((state, action))\n",
    "\n",
    "    def run_mc_control(self, num_episodes):\n",
    "        '''Performs Monte Carlo control task.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        num_episodes: integer, number of episodes to run to train RL agent\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        self.Q:              nested dictionary {state: {action: q value}}, final action value function\n",
    "        self.policy:         list of integers of length self.num_states, final policy\n",
    "        rewards_per_episode: numpy array of rewards collected at each episode\n",
    "        '''\n",
    "        self.init_agent()\n",
    "        \n",
    "        rewards_per_episode = np.array([None] * num_episodes)\n",
    "\n",
    "        for episode in range(num_episodes):\n",
    "            G, state_action_reward = self.generate_episode(self.policy)\n",
    "            self.evaluate_policy(G, state_action_reward)\n",
    "            self.improve_policy()\n",
    "            rewards_per_episode[episode] = G\n",
    "\n",
    "        print (f\"Finished training RL agent for {num_episodes} episodes!\")\n",
    "        \n",
    "        return self.Q, self.policy, rewards_per_episode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test init_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "epsilon = 0.4\n",
    "gamma = 0.9\n",
    "n_episodes = 10000\n",
    "\n",
    "env = None\n",
    "num_states = 2\n",
    "num_actions = 3\n",
    "\n",
    "mc_model = MCControl(env, num_states, num_actions, epsilon, gamma)\n",
    "\n",
    "mc_model.init_agent()\n",
    "assert np.all(mc_model.policy == np.array([1, 0]))\n",
    "assert mc_model.Q == {0: {0: 0, 1: 0, 2: 0}, 1: {0: 0, 1: 0, 2: 0}}\n",
    "assert mc_model.visit_count == {0: {0: 0, 1: 0, 2: 0}, 1: {0: 0, 1: 0, 2: 0}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test generate_episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-9d1e48049192>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmc_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mres\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "epsilon = 0.4\n",
    "gamma = 0.9\n",
    "n_episodes = 10000\n",
    "\n",
    "env = gym.make('FrozenLake-v0')\n",
    "env.seed(2)\n",
    "num_states = env.observation_space.n\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "\n",
    "mc_model = MCControl(env, num_states, num_actions, epsilon, gamma)\n",
    "\n",
    "policy = np.array([1, 1, 1, 1, 0, 0, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3])\n",
    "res = mc_model.generate_episode(policy)\n",
    "\n",
    "assert res == (0.0, [(0, 1, 0), (4, 0, 0.0), (4, 3, 0.0), (4, 0, 0.0), (8, 1, 0.0)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize environment and Monte Carlo Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_single(env, policy, max_steps=100):\n",
    "    \"\"\"\n",
    "    This function does not need to be modified\n",
    "    Renders policy once on environment. Watch your agent play!\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env: gym.core.Environment\n",
    "      Environment to play on. Must have nS, nA, and P as\n",
    "      attributes.\n",
    "    Policy: np.array of shape [env.nS]\n",
    "      The action to take at a given state\n",
    "    \"\"\"\n",
    "    episode_reward = 0\n",
    "    ob = env.reset()\n",
    "    for t in range(max_steps):\n",
    "        env.render()\n",
    "        time.sleep(0.25)\n",
    "        a = policy[ob]\n",
    "        ob, reward, done, _ = env.step(a)\n",
    "        episode_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "    env.render();\n",
    "    if not done:\n",
    "        print(\"The agent didn't reach a terminal state in {} steps.\".format(max_steps))\n",
    "    else:\n",
    "        print(\"Episode reward: %f\" % episode_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take MC Control to a Frozen Lake!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training RL agent for 2000 episodes!\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "epsilon = 0.4\n",
    "gamma = 1.0\n",
    "n_episodes = 2000\n",
    "\n",
    "env.seed(0)\n",
    "env = gym.make('FrozenLake-v0', is_slippery=False)\n",
    "\n",
    "num_states = env.observation_space.n\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "mc_model = MCControl(env, num_states, num_actions, epsilon, gamma)\n",
    "\n",
    "Q, policy, rewards_per_episode = mc_model.run_mc_control(n_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Episode reward: 1.000000\n"
     ]
    }
   ],
   "source": [
    "render_single(env, policy, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "epsilon = 0.4\n",
    "gamma = 1.0\n",
    "n_episodes = 2000\n",
    "\n",
    "num_runs = 10\n",
    "\n",
    "env = gym.make('FrozenLake-v0', is_slippery=False)\n",
    "num_states = env.observation_space.n\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "# every row is the record of rewards by episide per unique run, e.g. rewards_matrix[0, 0] is the rewards obtained in episode 1 of run 1\n",
    "rewards_matrix = np.zeros()\n",
    "    \n",
    "for run in range(num_runs):\n",
    "    env.seed(run)\n",
    "    mc_model = MCControl(env, num_states, num_actions, epsilon, gamma)\n",
    "    Q, policy, rewards_per_episode = mc_model.run_mc_control(n_episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Sum of rewards vs number of episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Cumulative sum of rewards over episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Episode length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Most visited states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
