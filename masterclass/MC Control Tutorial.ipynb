{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Control from scratch in Python and solving Frozen Lake problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pseudocode**:\n",
    "    \n",
    "    \n",
    "Input:  $epsilon$, $gamma$, $n\\_episodes$\n",
    "\n",
    "\n",
    "Initialize for all $s\\in S$ and $a\\in A$:    \n",
    ">$Q(s, a)$ <- arbitrary  \n",
    "    $\\pi(s)$ <- arbitrary\n",
    "\n",
    "Repeat for $n\\_episodes$:  \n",
    ">generate episode using exploring starts and current policy $\\pi$  \n",
    "    $Q(s, a)$ <- evaluate policy  \n",
    "    $\\pi$ <- improve policy\n",
    "\n",
    " \n",
    "$Q^*(s, a)$ <- $Q(s, a)$  \n",
    "$\\pi^*$  <- $\\pi$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import itertools\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We will create class called MC control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCControl:\n",
    "    def __init__(self, env, num_states, num_actions, epsilon, gamma):\n",
    "        self.env = env\n",
    "        self.num_states = num_states\n",
    "        self.num_actions = num_actions\n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def init_agent(self):\n",
    "        self.policy = np.random.choice(num_actions, num_states)\n",
    "\n",
    "        self.Q = {}\n",
    "        self.visit_count = {}\n",
    "\n",
    "        for state in range(self.num_states):\n",
    "            self.Q[state] = {}\n",
    "            self.visit_count[state] = {}\n",
    "            for action in range(self.num_actions):\n",
    "                # --------------------------\n",
    "                # Calculate G:\n",
    "                # your code here (~ 4 lines)\n",
    "\n",
    "                # --------------------------\n",
    "                self.Q[state][action] = 0\n",
    "                self.visit_count[state][action] = 0\n",
    "\n",
    "    def get_epsilon_greedy_action(self, greedy_action):\n",
    "        \n",
    "        prob = np.random.random()\n",
    "\n",
    "        if prob < 1 - self.epsilon:\n",
    "            return greedy_action\n",
    "\n",
    "        return np.random.randint(0, self.num_actions)\n",
    "\n",
    "    def generate_episode(self, policy):\n",
    "        '''Generates episode given current policy.\n",
    "        Parameters\n",
    "        ----------\n",
    "        policy: list of integers of length self.num_states, the action to take at a given state\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        G: float, episode return (total discounted reward)\n",
    "        state_action_reward: list of tuple (state, action, reward), excludes terminal one\n",
    "        '''\n",
    "        G = 0\n",
    "        s = env.reset()\n",
    "        a = self.get_epsilon_greedy_action(policy[s])\n",
    "\n",
    "        state_action_reward = [(s, a, 0)]\n",
    "        while True:\n",
    "            s, r, terminated, _ = env.step(a)\n",
    "            if terminated:\n",
    "                state_action_reward.append((s, None, r))\n",
    "                break\n",
    "            else:\n",
    "                a = self.get_epsilon_greedy_action(policy[s])\n",
    "                state_action_reward.append((s, a, r))\n",
    "\n",
    "        # --------------------------\n",
    "        # Calculate G:\n",
    "        # your code here (~ 4 lines)\n",
    " \n",
    "        # --------------------------\n",
    "\n",
    "        return G, state_action_reward[:-1]\n",
    "\n",
    "    def argmax(self, Q, policy):\n",
    "        \"\"\"\n",
    "        Finds and returns greedy policy.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        Q: nested dictionary {state: {action: q value}}, action value function\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        policy: list of integers of length self.num_states, the action to take at a given state \n",
    "\n",
    "        \"\"\"\n",
    "        greedy_policy = ???\n",
    "        for state in range(self.num_states):\n",
    "            # --------------------------\n",
    "            # Find greedy action to take in every state and assign to policy[state]:\n",
    "            # your code here (~ 4 lines)\n",
    "            best_action = None\n",
    "            best_value = float('-inf')\n",
    "\n",
    "            for action, value in Q[state].items():\n",
    "                if value > best_value:\n",
    "                    best_value = value\n",
    "                    best_action = action\n",
    "            policy[state] = best_action\n",
    "            # --------------------------\n",
    "\n",
    "        return policy\n",
    "\n",
    "    def improve_policy(self):\n",
    "        '''Improves and updates current policy self.policy.\n",
    "        '''\n",
    "        self.policy = self.argmax(self.Q, self.policy) \n",
    "        \n",
    "    def evaluate_policy(self, G, state_action_reward):\n",
    "        '''Evaluates current policy self.Q using incremental mean.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        G: float, episode return (total discounted reward)\n",
    "        state_action_reward: list of tuple (state, action, reward)\n",
    "        '''\n",
    "        seen_state_action = set()\n",
    "\n",
    "        for state, action, _ in state_action_reward:\n",
    "            #  if we see step and action pair for a first time in episode\n",
    "            if (state, action) not in seen_state_action:\n",
    "                self.visit_count[state][action] += 1\n",
    "                # --------------------------\n",
    "                # Calculate action value for current step and action\n",
    "                # your code here\n",
    "                self.Q[step][action] += (G - self.Q[step][action]) / self.visit_count[step][action]\n",
    "                # --------------------------\n",
    "                seen_state_action.add((state, action))\n",
    "\n",
    "    def run_mc_control(self, num_episodes):\n",
    "        '''Performs Monte Carlo control task.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        num_episodes: integer, number of episodes to run to train RL agent\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        self.Q:      nested dictionary {state: {action: q value}}, final action value function\n",
    "        self.policy: list of integers of length self.num_states, final policy\n",
    "        '''\n",
    "        self.init_agent()\n",
    "\n",
    "        for episode in range(num_episodes):\n",
    "            G, state_action_reward = self.generate_episode(self.policy)\n",
    "            self.evaluate_policy(G, state_action_reward)\n",
    "            self.improve_policy()\n",
    "\n",
    "        print (f\"Finished training RL agent for {num_episodes} episodes!\")\n",
    "        \n",
    "        return self.Q, self.policy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Fully implemented class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCControl:\n",
    "    def __init__(self, epsilon, gamma, env, num_states, num_actions):\n",
    "        # Fully implemented\n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = gamma\n",
    "        self.env = env\n",
    "        self.num_states = num_states\n",
    "        self.num_actions = num_actions\n",
    "\n",
    "\n",
    "    def init_agent(self):\n",
    "        self.policy = np.random.choice(num_actions, num_states)\n",
    "\n",
    "        self.Q = {}\n",
    "        self.visit_count = {}\n",
    "\n",
    "        for state in range(self.num_states):\n",
    "            self.Q[state] = {}\n",
    "            self.visit_count[state] = {}\n",
    "            for action in range(self.num_actions):\n",
    "                self.Q[state][action] = 0\n",
    "                self.visit_count[state][action] = 0\n",
    "\n",
    "\n",
    "    def get_epsilon_greedy_action(self, greedy_action):\n",
    "        prob = np.random.random()\n",
    "\n",
    "        if prob < 1 - self.epsilon:\n",
    "            return greedy_action\n",
    "\n",
    "        return np.random.randint(0, self.num_actions)\n",
    "\n",
    "\n",
    "    def generate_episode(self, policy):\n",
    "        # Partially implemented\n",
    "        G = 0\n",
    "        s = env.reset()\n",
    "        a = self.get_epsilon_greedy_action(policy[s])\n",
    "\n",
    "        state_action_reward = [(s, a, 0)]\n",
    "        while True:\n",
    "            s, r, terminated, _ = env.step(a)\n",
    "            if terminated:\n",
    "                state_action_reward.append((s, None, r))\n",
    "                break\n",
    "            else:\n",
    "                a = self.get_epsilon_greedy_action(policy[s])\n",
    "                state_action_reward.append((s, a, r))\n",
    "\n",
    "        t = 1\n",
    "        for _, _, reward in state_action_reward:\n",
    "            G += self.gamma ** (t - 1) * reward\n",
    "            t += 1\n",
    "\n",
    "        return G, state_action_reward[:-1]\n",
    "\n",
    "    def argmax(self, Q, policy):\n",
    "        \"\"\"\n",
    "        Finds and returns greedy policy.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        Q: nested dictionary {state: {action: q value}}\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        policy: The action to take at a given state, list of length num_state\n",
    "\n",
    "        \"\"\"\n",
    "        for state in range(self.num_states):\n",
    "            best_action = None\n",
    "            best_value = float('-inf')\n",
    "\n",
    "            for action, value in Q[state].items():\n",
    "                if value > best_value:\n",
    "                    best_value = value\n",
    "                    best_action = action\n",
    "            policy[state] = best_action\n",
    "\n",
    "        return policy\n",
    "\n",
    "    def evaluate_policy(self, G, visit_counts, step, action):\n",
    "        self.Q[step][action] += (G - self.Q[step][action]) / visit_counts[step][action]\n",
    "\n",
    "\n",
    "    def improve_policy(self, Q, policy):\n",
    "        self.policy = self.argmax(Q, policy)\n",
    "\n",
    "\n",
    "    def run_mc_control(self, num_episodes):\n",
    "        # Fully implemented\n",
    "        self.init_agent()\n",
    "\n",
    "        for episode in range(num_episodes):\n",
    "            G, state_action_reward = self.generate_episode(self.policy)\n",
    "            seen_state_action = set()\n",
    "\n",
    "            for state, action, _ in state_action_reward:\n",
    "                #  if we see step and action pair for a first time in episode\n",
    "                if (state, action) not in seen_state_action:\n",
    "                    self.visit_count[state][action] += 1\n",
    "\n",
    "                    self.evaluate_policy(G, self.visit_count, state, action)\n",
    "\n",
    "                    seen_state_action.add((state, action))\n",
    "\n",
    "            self.improve_policy(self.Q, self.policy)\n",
    "\n",
    "        print (f\"Finished training RL agent for {num_episodes} episodes!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Train RL agent and test it on the Frozen Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_single(env, policy, max_steps=100):\n",
    "    \"\"\"\n",
    "    This function does not need to be modified\n",
    "    Renders policy once on environment. Watch your agent play!\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env: gym.core.Environment\n",
    "      Environment to play on. Must have nS, nA, and P as\n",
    "      attributes.\n",
    "    Policy: np.array of shape [env.nS]\n",
    "      The action to take at a given state\n",
    "    \"\"\"\n",
    "    episode_reward = 0\n",
    "    ob = env.reset()\n",
    "    for t in range(max_steps):\n",
    "        env.render()\n",
    "        time.sleep(0.25)\n",
    "        a = policy[ob]\n",
    "        ob, reward, done, _ = env.step(a)\n",
    "        episode_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "    env.render();\n",
    "    if not done:\n",
    "        print(\"The agent didn't reach a terminal state in {} steps.\".format(max_steps))\n",
    "    else:\n",
    "        print(\"Episode reward: %f\" % episode_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.4\n",
    "gamma = 0.9\n",
    "n_episodes = 1000\n",
    "\n",
    "env = gym.make('FrozenLake-v0')\n",
    "\n",
    "num_states = env.observation_space.n\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "mc_model = MCControl(env, num_states, num_actions, epsilon, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training RL agent for 1000 episodes!\n"
     ]
    }
   ],
   "source": [
    "Q, policy = mc_model.run_mc_control(n_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "arrays used as indices must be of integer (or boolean) type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-7c7803c12826>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrender_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmc_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-da89b5f825bd>\u001b[0m in \u001b[0;36mrender_single\u001b[0;34m(env, policy, max_steps)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mob\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mepisode_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: arrays used as indices must be of integer (or boolean) type"
     ]
    }
   ],
   "source": [
    "render_single(env, policy, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bipedal Walker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "epsilon = 0.4\n",
    "gamma = 0.9\n",
    "n_episodes = 1000\n",
    "\n",
    "env = gym.make('BipedalWalker-v2')\n",
    "\n",
    "num_states = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.shape[0]\n",
    "\n",
    "mc_model = MCControl(epsilon, gamma, env, num_states, num_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "arrays used as indices must be of integer (or boolean) type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-5fc852062f58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmc_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_mc_control\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-da89b5f825bd>\u001b[0m in \u001b[0;36mrun_mc_control\u001b[0;34m(self, num_episodes)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepisode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m             \u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_action_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m             \u001b[0mseen_state_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-da89b5f825bd>\u001b[0m in \u001b[0;36mgenerate_episode\u001b[0;34m(self, policy)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_epsilon_greedy_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mstate_action_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: arrays used as indices must be of integer (or boolean) type"
     ]
    }
   ],
   "source": [
    "mc_model.run_mc_control(n_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
